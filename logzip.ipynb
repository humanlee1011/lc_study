{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logzip empirical study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../logzip\")\n",
    "from logzip.logzipper import Ziplog\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import lzma\n",
    "import os\n",
    "import shutil\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path dir\n",
    "from genericpath import exists\n",
    "\n",
    "\n",
    "logs_path = \"../logs\"\n",
    "template_filepath = \"../logs\"\n",
    "output_dir = \"./zip_out\"\n",
    "tmp_dir = os.path.join(output_dir, \"tmp_dir\")\n",
    "result_path = \"./results\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "logformat = {\n",
    "    \"HDFS\": \"<Date> <Time> <Pid> <Level> <Component>: <Content>\",\n",
    "    \"Hadoop\": \"<Date> <Time> <Level> \\[<Process>\\] <Component>: <Content>\",\n",
    "    \"Spark\": \"<Date> <Time> <Level> <Component>: <Content>\",\n",
    "    \"Zookeeper\": \"<Date> <Time> - <Level>  \\[<Node>:<Component>@<Id>\\] - <Content>\",\n",
    "    \"BGL\": \"<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>\",\n",
    "    \"HPC\": \"<LogId> <Node> <Component> <State> <Time> <Flag> <Content>\",\n",
    "    \"Thunderbird\": \"<Label> <Timestamp> <Date> <User> <Month> <Day> <Time> <Location> <Component>(\\[<PID>\\])?: <Content>\",\n",
    "    \"Windows\": \"<Date> <Time>, <Level>                  <Component>    <Content>\",\n",
    "    \"Linux\": \"<Month> <Date> <Time> <Level> <Component>(\\[<PID>\\])?: <Content>\",\n",
    "    \"Andriod\": \"<Date> <Time>  <Pid>  <Tid> <Level> <Component>: <Content>\",\n",
    "    \"HealthApp\": \"<Time>\\|<Component>\\|<Pid>\\|<Content>\",\n",
    "    \"Apache\": \"\\[<Time>\\] \\[<Level>\\] <Content>\",\n",
    "    \"Proxifier\": \"\\[<Time>\\] <Program> - <Content>\",\n",
    "    \"OpenSSH\": \"<Date> <Day> <Time> <Component> sshd\\[<Pid>\\]: <Content>\",\n",
    "    \"OpenStack\": \"<Logrecord> <Date> <Time> <Pid> <Level> <Component> \\[<ADDR>\\] <Content>\",\n",
    "    \"Mac\": \"<Month>  <Date> <Time> <User> <Component>\\[<PID>\\]( \\(<Address>\\))?: <Content>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_compression_ratio(infile, outfile):\n",
    "    return os.path.getsize(infile) / os.path.getsize(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logzip_(n_workers, logformat, out_dir, outname, kernel, tmp_dir, level, in_filepath, template_filepath):\n",
    "    zipper = Ziplog(logformat=logformat,\n",
    "                    outdir=out_dir,\n",
    "                    outname=outname,\n",
    "                    kernel=kernel,\n",
    "                    tmp_dir=tmp_dir,\n",
    "                    level=level)\n",
    "    zipper.zip_file(in_filepath, template_filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logzip(mode=\"all\", kernel=\"bz2\"):\n",
    "    # basic setting of logzip\n",
    "    level = 1\n",
    "    n_workers = 1\n",
    "    results = dict()\n",
    "\n",
    "    if mode == \"all\":\n",
    "        dataset = os.listdir(logs_path)\n",
    "    else:\n",
    "        dataset = mode\n",
    "\n",
    "    for ds in dataset:\n",
    "        # define log format\n",
    "        out_dir = os.path.join(output_dir, ds)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "        outname = ds + '.logzip'\n",
    "        in_filepath = os.path.join(logs_path, ds, ds + '_2k.log')\n",
    "        template_filepath = os.path.join(logs_path, ds, ds + '_2k.log_templates.csv')\n",
    "        # (n_workers, logformat, out_dir, outname, kernel, tmp_dir, level, in_filepath, template_filepath)\n",
    "        run_logzip_(n_workers, logformat[ds], out_dir, outname, kernel, tmp_dir, level, in_filepath, template_filepath)\n",
    "        output_path = os.path.join(output_dir, ds, outname + '.tar.' + kernel)\n",
    "        print(\"input file \", in_filepath, output_path)\n",
    "        results[ds] = evaluate_compression_ratio(in_filepath, output_path)\n",
    "    \n",
    "    pickle.dump(results, open(os.path.join(result_path, \"logzip_\" + kernel + \".pkl\"), 'wb'))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading log messages to dataframe...\n",
      "Total lines 2000\n",
      "Worker 70704 processing.\n",
      "Worker 70704 finish.\n",
      "Loading 2000 messages done, loading rate: 100.0%, failed lines: 0\n",
      "Time taken 0.02s\n",
      "Processing log file: None...\n",
      "Building match tree...\n",
      "Matching event templates...\n",
      "Worker 70704 start matching 2000 lines.\n",
      "Matching done, matching rate: 0.0% [Time taken: 0:00:00.065413]\n"
     ]
    }
   ],
   "source": [
    "# run_logzip(mode=[\"HDFS\"], kernel=\"gz\")\n",
    "\n",
    "out_dir = \"./zip_out/\"\n",
    "outname = \"HDFS_2k\" + \".logzip\"\n",
    "tmp_dir = \"./zip_out/tmp_dir\"\n",
    "\n",
    "level = 1\n",
    "kernel = \"bz2\"   # options: (1) gz  (2) bz2\n",
    "n_workers = 1\n",
    "\n",
    "zipper = Ziplog(logformat=\"<Date> <Time> <Pid> <Level> <Component>: <Content>\",\n",
    "                outdir=out_dir,\n",
    "                outname=outname,\n",
    "                kernel=kernel,\n",
    "                tmp_dir=tmp_dir,\n",
    "                level=level)\n",
    "zipper.zip_file(\"../logs/HDFS/HDFS_2k.log\", \"../logs/HDFS/HDFS_2k.log_templates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baselines(mode=\"all\", kernel=\"gz\"):\n",
    "    results = dict()\n",
    "\n",
    "    for ds in os.listdir(logs_path):\n",
    "        out_dir = os.path.join(output_dir, ds)\n",
    "        output_name = os.path.join(output_dir, ds, ds + \".\" + kernel)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "        source_dir = os.path.join(logs_path, ds, ds + '_2k.log')\n",
    "\n",
    "        if kernel in [\"gz\", \"bz2\"]:\n",
    "            with tarfile.open(output_name, \"w:gz\") as tar:\n",
    "                tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "        elif kernel in [\"lzma\"]:\n",
    "            with open(source_dir, 'rb') as f, open(output_name, 'wb') as out:\n",
    "                out.write(lzma.compress(bytes(f.read())))\n",
    "        \n",
    "        results[ds] = evaluate_compression_ratio(source_dir, output_name)\n",
    "    \n",
    "    pickle.dump(results, open(os.path.join(result_path, \"tar_\" + kernel + \".pkl\"), 'wb'))\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spark': 19.591367486889876,\n",
       " 'Apache': 23.337010479867622,\n",
       " 'Mac': 9.151626110021912,\n",
       " 'HealthApp': 13.885669362084457,\n",
       " 'Linux': 18.395025728987992,\n",
       " 'HDFS': 6.693078580125503,\n",
       " 'Hadoop': 29.17039914686167,\n",
       " 'OpenStack': 14.148854961832061,\n",
       " 'Proxifier': 13.193875278396437,\n",
       " 'Andriod': 16.50846043851287,\n",
       " 'BGL': 7.927928154558261,\n",
       " 'OpenSSH': 18.852787162162162,\n",
       " 'Zookeeper': 15.904990842490843,\n",
       " 'Thunderbird': 15.577067669172932,\n",
       " 'HPC': 7.838272383354351,\n",
       " 'Windows': 30.489888123924267}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_baselines(mode=\"all\", kernel=\"gz\")\n",
    "run_baselines(mode=\"all\", kernel=\"bz2\")\n",
    "run_baselines(mode=\"all\", kernel=\"lzma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading log messages to dataframe...\n",
      "Total lines 2000\n",
      "Worker 70704 processing.\n",
      "Worker 70704 finish.\n",
      "Loading 2000 messages done, loading rate: 100.0%, failed lines: 0\n",
      "Time taken 0.03s\n",
      "Processing log file: None...\n",
      "Building match tree...\n",
      "Matching event templates...\n",
      "Worker 70704 start matching 2000 lines.\n",
      "Matching done, matching rate: 0.0% [Time taken: 0:00:00.092982]\n",
      "input file  ../logs/HDFS/HDFS_2k.log ./zip_out/HDFS/HDFS.logzip.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HDFS': 5.433856097329151}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logzip(mode=[\"HDFS\"], kernel=\"gz\")\n",
    "# run_logzip(kernel=\"bz2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91455c23e131deb582773966e6714d1f9b3e27f2de3399b4028c5cb01858b302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
